# -*- coding: utf-8 -*-
"""NLP_PIPELINE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mnSTvC6acGR4WSx6bFFwYhNRVYYJ0Nk-
"""

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk




# 1 : Preprocesing Text
# Créer un corpus qui contient les textes d1,d2,d3,d4,d5

corpus = {
    "d1":"Le chat dort sur le tapis",
    "d2":"Les Oiseaux Chantent Le Matin",
    "d3":"Le chien court dans le jardin",
    "d4":"Mangeons des pommes délicieuses",
    "d5":"Je mange une orange fraiche"
}


# Convertir le corpus en type DataFrame
corpus_df=pd.DataFrame(list(corpus.items()),columns=["Corpus","Text"])


# Afficher le corpus en forme d une dataFrame
corpus_df

import string

# Afficher ponctuation
print(string.punctuation)


def code(texte):
    text_without_pon=[c for c in texte if c not in string.punctuation]
    return "".join(text_without_pon)


# print(code('salam @ soufiane'))
corpus_list=list(corpus.values().mapping.values())
corpus_df=pd.DataFrame(list(corpus.values()),columns=["Text"])



corpus_list_pon = [code(item) for item in corpus_list]
corpus_df['Text_Without_Pon']=corpus_list_pon

corpus_df

def tokenizer(liste):
    corpus_tokenize=list()
    for text in liste:
        corpus_tokenize.append(word_tokenize(text))
    return corpus_tokenize

from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('french'))

print(stop_words)

corpus_tokenize=tokenizer(corpus_list_pon)

corpus_df['Text_Tokenize']=corpus_tokenize

corpus_df

# filtered_text = [word for word in words if word.lower() not in stop_words]

# print(filtered_text)

def elimnier_stop_words(liste):

    corpus_s_stopwords=list()

    for words in liste:
        filtered_text =[word for word in words if word.lower() not in stop_words]

        corpus_s_stopwords.append(filtered_text)
    return corpus_s_stopwords

# Eliminer stopwords de corpus
elimnier_stop_words(corpus_tokenize)

corpus_df["Text_Without_Stopwords"]=elimnier_stop_words(corpus_tokenize)

# Affichier le corpus DataFrame

corpus_df

nltk.download('wordnet')
from nltk.stem.snowball import FrenchStemmer

def lemmatize_text(text):
    lemmatizer = FrenchStemmer()
    tokens = word_tokenize(text,language='french')
    lemmatized_tokens = [lemmatizer.stem(token) for token in tokens]
    return ' '.join(lemmatized_tokens)

# lemmatize_text('les enfants') ca va nous affiche  le enfant

carpus_lemmitizer=list()

for words in corpus_list_pon:

    carpus_lemmitizer.append(lemmatize_text(words))

corpus_df['Text_Lemmitizer']=carpus_lemmitizer

corpus_df